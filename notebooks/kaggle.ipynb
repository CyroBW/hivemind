{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install \"tensorflow[and-cuda]\"\n",
    "!pip install \"flax[all]\"\n",
    "!pip install gdown\n",
    "!pip install pgx\n",
    "!pip install git+https://github.com/aminwoo/pgx.git\n",
    "!pip install mctx\n",
    "!pip install tqdm\n",
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/drive/folders/17-BtU1koT2nulH6NzsQum9fTQCMGuN2g?usp=sharing -O /kaggle/working/ --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def mish(x):\n",
    "    return x * jnp.tanh(jax.nn.softplus(x))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AZResnetConfig:\n",
    "    num_blocks: int\n",
    "    channels: int\n",
    "    policy_channels: int\n",
    "    value_channels: int\n",
    "    num_policy_labels: int\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    channels: int\n",
    "    se: bool\n",
    "    se_ratio: int = 4\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        y = nn.Conv(\n",
    "            features=self.channels, kernel_size=(3, 3), padding=(1, 1), use_bias=False\n",
    "        )(x)\n",
    "        y = nn.BatchNorm(use_running_average=not train)(y)\n",
    "        y = mish(y)\n",
    "        y = nn.Conv(\n",
    "            features=self.channels, kernel_size=(3, 3), padding=(1, 1), use_bias=False\n",
    "        )(x)\n",
    "        y = nn.BatchNorm(use_running_average=not train)(y)\n",
    "\n",
    "        if self.se:\n",
    "            squeeze = jnp.mean(y, axis=(1, 2), keepdims=True)\n",
    "\n",
    "            excitation = nn.Dense(\n",
    "                features=self.channels // self.se_ratio, use_bias=True\n",
    "            )(squeeze)\n",
    "            excitation = nn.relu(excitation)\n",
    "            excitation = nn.Dense(features=self.channels, use_bias=True)(excitation)\n",
    "            excitation = nn.hard_sigmoid(excitation)\n",
    "\n",
    "            y = y * excitation\n",
    "\n",
    "        return mish(x + y)\n",
    "\n",
    "\n",
    "class AZResnet(nn.Module):\n",
    "    config: AZResnetConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = nn.Conv(\n",
    "            features=self.config.channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            use_bias=False,\n",
    "        )(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = mish(x)\n",
    "\n",
    "        for _ in range(self.config.num_blocks):\n",
    "            x = ResidualBlock(channels=self.config.channels, se=True)(x, train=train)\n",
    "\n",
    "        # policy head\n",
    "        policy = nn.Conv(\n",
    "            features=self.config.channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            use_bias=False,\n",
    "        )(x)\n",
    "        policy = nn.BatchNorm(use_running_average=not train)(policy)\n",
    "        policy = mish(policy)\n",
    "        policy = nn.Conv(\n",
    "            features=self.config.policy_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            use_bias=False,\n",
    "        )(policy)\n",
    "        policy = nn.BatchNorm(use_running_average=not train)(policy)\n",
    "        policy = mish(policy)\n",
    "        policy = policy.reshape((batch_size, -1))\n",
    "        policy = nn.Dense(features=self.config.num_policy_labels)(policy)\n",
    "\n",
    "        # value head\n",
    "        value = nn.Conv(\n",
    "            features=self.config.value_channels, kernel_size=(1, 1), use_bias=False\n",
    "        )(x)\n",
    "        value = nn.BatchNorm(use_running_average=not train)(value)\n",
    "        value = mish(value)\n",
    "        value = value.reshape((batch_size, -1))\n",
    "        value = nn.Dense(features=256)(value)\n",
    "        value = mish(value)\n",
    "        value = nn.Dense(features=1)(value)\n",
    "        value = nn.tanh(value)\n",
    "        value = value.squeeze(axis=1)\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import Any\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import orbax\n",
    "import orbax.checkpoint as ocp\n",
    "import chex\n",
    "\n",
    "from flax.training import orbax_utils\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: chex.ArrayTree\n",
    "\n",
    "\n",
    "class TrainerModule:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: nn.Module,\n",
    "        model_configs: Any,\n",
    "        optimizer_name: str,\n",
    "        optimizer_params: dict,\n",
    "        x: Any,\n",
    "        ckpt_dir: str = '/kaggle/working/',\n",
    "        max_checkpoints: int = 99,\n",
    "        seed=42,\n",
    "    ):\n",
    "        '''\n",
    "        Module for summarizing all training functionalities for classification.\n",
    "\n",
    "        Inputs:\n",
    "            model_name - String of the class name, used for logging and saving\n",
    "            model_class - Class implementing the neural network\n",
    "            model_hparams - Hyperparameters of the model, used as input to model constructor\n",
    "            optimizer_name - String of the optimizer name, supporting ['sgd', 'adam', 'adamw']\n",
    "            optimizer_params - Hyperparameters of the optimizer, including learning rate as 'lr'\n",
    "            x - Example imgs, used as input to initialize the model\n",
    "            seed - Seed to use in the model initialization\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model_class = model_class\n",
    "        self.model_configs = model_configs\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.seed = seed\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = self.model_class(model_configs)\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        \n",
    "        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        options = orbax.checkpoint.CheckpointManagerOptions(\n",
    "            max_to_keep=max_checkpoints, create=True\n",
    "        )\n",
    "        self.checkpoint_manager = orbax.checkpoint.CheckpointManager(\n",
    "            self.ckpt_dir, orbax_checkpointer, options\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, step=0) -> TrainState:\n",
    "        # Load model. We use different checkpoint for pretrained models\n",
    "        ckpt = self.checkpoint_manager.restore(step)\n",
    "        self.state = ckpt['train_state']\n",
    "        return ckpt['train_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import random \n",
    "import time\n",
    "import requests\n",
    "from threading import Thread\n",
    "from functools import partial\n",
    "from typing import Optional, List\n",
    "\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np \n",
    "import mctx\n",
    "import optax\n",
    "import pgx\n",
    "from flax.training import train_state\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Sample(BaseModel):\n",
    "    obs: List[List[float]]\n",
    "    policy_tgt: List[List[float]]\n",
    "    value_tgt: List[int]\n",
    "\n",
    "\n",
    "model_configs = AZResnetConfig(\n",
    "    num_blocks=15,\n",
    "    channels=256,\n",
    "    policy_channels=4,\n",
    "    value_channels=8,\n",
    "    num_policy_labels=2*64*78+1,\n",
    ")\n",
    "net = AZResnet(model_configs)\n",
    "trainer = TrainerModule(model_class=AZResnet, model_configs=model_configs, optimizer_name='lion', optimizer_params={'learning_rate': 1}, x=jnp.ones((1, 8, 16, 32)))\n",
    "state = trainer.load_checkpoint(20240406121656)\n",
    "\n",
    "params = {'params': state['params'], 'batch_stats': state['batch_stats']}\n",
    "forward = jax.jit(partial(net.apply, train=False))\n",
    "\n",
    "devices = jax.local_devices()\n",
    "num_devices = len(devices)\n",
    "print('Number of devices:', num_devices)\n",
    "\n",
    "class Config(BaseModel):\n",
    "    env_id: pgx.EnvId = 'bughouse'\n",
    "    seed: int = random.randint(0, 999999999)\n",
    "    max_num_iters: int = 1000\n",
    "    # selfplay params\n",
    "    selfplay_batch_size: int = 1\n",
    "    num_simulations: int = 800\n",
    "    max_num_steps: int = 512\n",
    "\n",
    "    class Config:\n",
    "        extra = 'forbid'\n",
    "\n",
    "config: Config = Config()\n",
    "\n",
    "env = pgx.make(config.env_id)\n",
    "\n",
    "def recurrent_fn(params, rng_key: jnp.ndarray, action: jnp.ndarray, state: pgx.State):\n",
    "    rng_keys = jax.random.split(rng_key, config.selfplay_batch_size)\n",
    "    current_player = state.current_player\n",
    "    state = jax.vmap(env.step)(state, action, rng_keys)\n",
    "\n",
    "    logits, value = forward(params, state.observation)\n",
    "    logits = logits.at[:, 9984].set(jnp.max(logits, axis=1))\n",
    "\n",
    "    # mask invalid actions\n",
    "    logits = logits - jnp.max(logits, axis=-1, keepdims=True)\n",
    "    logits = jnp.where(state.legal_action_mask, logits, jnp.finfo(logits.dtype).min)\n",
    "\n",
    "    reward = state.rewards[jnp.arange(state.rewards.shape[0]), current_player]\n",
    "    value = jnp.where(state.terminated, 0.0, value)\n",
    "    discount = -1.0 * jnp.ones_like(value)\n",
    "    discount = jnp.where(state.terminated, 0.0, discount)\n",
    "\n",
    "    recurrent_fn_output = mctx.RecurrentFnOutput(\n",
    "        reward=reward,\n",
    "        discount=discount,\n",
    "        prior_logits=logits,\n",
    "        value=value,\n",
    "    )\n",
    "    return recurrent_fn_output, state\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def run_mcts(state, key, num_simulations: int, tree: Optional[mctx.Tree] = None):\n",
    "    key1, key2 = jax.random.split(key)\n",
    "\n",
    "    logits, value = forward(params, state.observation)\n",
    "    logits = logits.at[:, 9984].set(jnp.max(logits, axis=1))\n",
    "\n",
    "    root = mctx.RootFnOutput(prior_logits=logits, value=value, embedding=state)\n",
    "\n",
    "    policy_output = mctx.alphazero_policy(\n",
    "        params=params,\n",
    "        rng_key=key1,\n",
    "        root=root,\n",
    "        recurrent_fn=recurrent_fn,\n",
    "        num_simulations=num_simulations,\n",
    "        invalid_actions=~state.legal_action_mask,\n",
    "        search_tree=None,\n",
    "        qtransform=partial(mctx.qtransform_by_min_max, min_value=-1, max_value=1),\n",
    "    )\n",
    "    return policy_output\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_fn = jax.jit(jax.vmap(env.init))\n",
    "    step_fn = jax.jit(jax.vmap(env.step))\n",
    "\n",
    "    print('Running selfplay with initial seed', config.seed)\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(config.seed)\n",
    "\n",
    "    for _ in tqdm(range(config.max_num_iters)):\n",
    "        game_id = random.randint(0, 999999999)\n",
    "        print(f'Playing game id: {game_id}')\n",
    "\n",
    "        rng_key, sub_key = jax.random.split(rng_key)\n",
    "        keys = jax.random.split(sub_key, config.selfplay_batch_size)\n",
    "        state = init_fn(keys)\n",
    "        tree = None \n",
    "        actions = [] \n",
    "        times = [] \n",
    "\n",
    "        obs = [] \n",
    "        policy_tgt = [] \n",
    "        value_tgt = [] \n",
    "\n",
    "        while ~state.terminated.all():\n",
    "            rng_key, sub_key = jax.random.split(rng_key)\n",
    "            policy_output = run_mcts(state, sub_key, config.num_simulations, tree)\n",
    "\n",
    "            obs.append(state.observation.ravel())\n",
    "            policy_tgt.append(policy_output.action_weights.ravel())\n",
    "\n",
    "            action = policy_output.action.item()\n",
    "\n",
    "            keys = jax.random.split(sub_key, config.selfplay_batch_size)\n",
    "            state = step_fn(state, policy_output.action, keys)\n",
    "\n",
    "        reward = abs(int(state.rewards[0][0]))\n",
    "        for i in range(len(obs)):\n",
    "            value_tgt.append(reward)\n",
    "            reward *= -1 \n",
    "\n",
    "        value_tgt = value_tgt[::-1]\n",
    "\n",
    "        game_data = {\n",
    "            \"obs\": obs, \n",
    "            \"policy_tgt\": policy_tgt, \n",
    "            \"value_tgt\": value_tgt,\n",
    "        }\n",
    "\n",
    "        url = f\"http://ec2-18-208-220-129.compute-1.amazonaws.com:8000/game/{game_id}/\"\n",
    "        sample = Sample(**game_data)\n",
    "        response = requests.post(url, json=sample.model_dump())\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Game sent successfully!\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
